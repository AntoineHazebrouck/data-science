{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Implantation de l’algorithme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Rappels\n",
    "\n",
    "L’algorithme K-means est un algorithme de clustering qui permet de partitionner automatiquement des données en sous-ensembles (clusters). L’objectif\n",
    "est de regrouper les données similaires au sein de sous-ensembles homogènes et\n",
    "de répartir les données différentes dans des sous-ensembles distincts.\n",
    "\n",
    "Pour un nombre de clusters K donné et à partir d’un ensemble de centres\n",
    "initiaux pour les K clusters, le principe consiste à itérer les deux opérations\n",
    "suivantes :\n",
    "- Affecter chaque donnée au cluster dont le centre est le plus proche (étape\n",
    "d’affectation)\n",
    "- Mettre à jour les centres des clusters en fonction des données présentes\n",
    "dans chaque cluster (étape de mise à jour des centres)\n",
    "\n",
    "Il existe plusieurs façons de calculer les centres (ou représentants) des clusters et la proximité des données. Le plus souvent, le centre est défini comme la\n",
    "moyenne des données du cluster et la distance considérée est la distance euclidienne. Nous allons procéder de cette façon dans le TP.\n",
    "\n",
    "Le nombre de clusters K est fixé a priori et l’initialisation des centres des clusters se fait de façon aléatoire, ce qui a pour conséquence de rendre l’algorithme\n",
    "non déterministe. L’initialisation a un impact important sur le partitionnement\n",
    "des données obtenu.\n",
    "\n",
    "## 1.2 Ecriture de l’algorithme\n",
    "\n",
    "La bibliothèque NumPy sera utilisée pour manipuler les vecteurs et procéder\n",
    "à des calculs vectoriels simples (addition, soustraction et multiplication).\n",
    "\n",
    "Pour implanter l’algorithme en Python et le rendre facilement réutilisable,\n",
    "nous allons nous inspirer de la bibliothèque Scikit-Learn, dans laquelle tous les\n",
    "algorithmes sont implantés en tant que classes d’objets.\n",
    "\n",
    "Vous devez écrire le code d’une classe KMeans dans un fichier que vous pourrez appeler kmeans.py. Cette classe doit comprendre les quatre méthodes suivantes :\n",
    "\n",
    "- la méthode spéciale __init__(self, dimension, max_iter, n_clusters)\n",
    "qui crée l’objet partitionneur et lui fournit les paramètres suivants :\n",
    "\t- dimension : dimension d des vecteurs d’entrée (nombre de caractéristiques de chaque entrée)\n",
    "\t- max_iter : nombre maximal d’itérations de l’algorithme\n",
    "\t- n_clusters : nombre K de clusters\n",
    "\n",
    "\tLa méthode doit aussi créer deux listes (vides) qui seront utilisées par\n",
    "l’algorithme K-means pour stocker les centres des clusters et le résultat\n",
    "de clustering. Ce résultat peut s’écrire sous la forme d’une liste d’entiers\n",
    "correspondant aux numéros des clusters affectés aux différents vecteurs\n",
    "d’entrée.\n",
    "\n",
    "- la méthode fit(self, X) qui réalise le clustering des données d’entrée\n",
    "X, définies sous la forme d’une liste de vecteurs NumPy de dimension\n",
    "dimension. Cette méthode ne retourne pas de résultat, elle correspond\n",
    "à l’algorithme K-means et réalise les étapes d’affectation des données aux\n",
    "clusters et de mise à jour des centres des clusters.\n",
    "\n",
    "\tRemarques :\n",
    "\t- Le nombre de données correspond à la taille de la liste X.\n",
    "\t- Pour l’initialisation, il est courant de choisir les centres des clusters\n",
    "parmi les données d’entrée (la liste qui stocke les centres sera de taille\n",
    "n_clusters).\n",
    "\t- L’affectation des données aux clusters peut se faire en ajoutant dans\n",
    "la liste correspondante le numéro du cluster le plus proche de chaque\n",
    "donnée (la liste qui stocke les numéros de cluster sera de même taille\n",
    "que X).\n",
    "\n",
    "- la méthode predict(self, x) qui retourne (prédit) le numéro du cluster\n",
    "le plus proche d’une donnée x, qui est un vecteur NumPy de dimension\n",
    "dimension.\n",
    "\n",
    "- la méthode get_data_clusters(self) qui retourne le résultat de clustering sous la forme d’une liste d’entiers contenant le numéro de cluster\n",
    "affecté à chaque donnée de la liste X.\n",
    "\n",
    "Pour bien comprendre comment les objets de cette classe peuvent être utilisés et faire une première validation de votre code, voici un exemple de session Python (à écrire dans un autre fichier, par exemple kmeans_test.py)\n",
    "qui crée une instance de la classe KMeans et utilise les méthodes fit() et\n",
    "get_data_clusters() pour réaliser le partitionnement des données et obtenir le résultat.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from kmeans import KMeans\n",
    "\n",
    "# creation des donnees d’entree\n",
    "# liste de 10 vecteurs de dimension 2 generes aleatoirement\n",
    "X = [np.random.random(2) ∗ 10 for i in range(0,10)]\n",
    "\n",
    "# creation du partitionneur\n",
    "k_means = KMeans(dimension=2,max_iter=100,n_clusters=2)\n",
    "\n",
    "# apprentissage −> realisation du partitionnement\n",
    "k_means.fit(X)\n",
    "\n",
    "# affichage des numeros de clusters affectes aux donnees\n",
    "print(k_means.get_data_clusters())\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.centroids [[ 1  3]\n",
      " [10 11]]\n",
      "data[:, np.newaxis].shape (6, 1, 2)\n",
      "data.shape (6, 2)\n",
      "self.centroids.shape (2, 2)\n",
      "data[:, np.newaxis].shape (6, 1, 2)\n",
      "data.shape (6, 2)\n",
      "self.centroids.shape (2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, dimension: int, max_iter: int, n_clusters: int):\n",
    "        self.dimension = dimension\n",
    "        self.max_iter = max_iter\n",
    "        self.n_clusters = n_clusters\n",
    "        # self.centroids\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.centroids = data[np.random.choice(data.shape[0], self.n_clusters, replace=False)]\n",
    "        print(\"self.centroids\", self.centroids)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            # TODO\n",
    "            \n",
    "             # 3. Assign each point to the closest centroid\n",
    "            distances = np.linalg.norm(data[:, np.newaxis] - self.centroids, axis=2)\n",
    "            print(\"data[:, np.newaxis].shape\", data[:, np.newaxis].shape)\n",
    "            print(\"data.shape\", data.shape)\n",
    "            print(\"self.centroids.shape\", self.centroids.shape)\n",
    "\n",
    "            labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "            # 4. Recompute centroids based on the mean of the points in each cluster\n",
    "            new_centroids = np.array([data[labels == i].mean(axis=0) for i in range(self.n_clusters)])\n",
    "        \n",
    "            # 5. If centroids don't change, break the loop\n",
    "            if np.all(self.centroids == new_centroids):\n",
    "                break\n",
    "\n",
    "            self.centroids = new_centroids\n",
    "    \n",
    "        # return self.centroids, labels\n",
    "\n",
    "    def predict(self, row):\n",
    "        # Calculate the distance from each new point to all centroids\n",
    "        distance = np.linalg.norm(row - self.centroids)\n",
    "    \n",
    "\n",
    "        # Get the index of the closest centroid for each new point\n",
    "        labels = np.argmin(distance, axis=0)\n",
    "    \n",
    "        return labels\n",
    "\n",
    "    def get_data_clusters(self):\n",
    "        pass\n",
    "\n",
    "data = np.array([[1, 2], [1, 3], [1, 4], [10, 10], [10, 11], [11, 10]])\n",
    "\n",
    "kmeans = KMeans(dimension=2,max_iter=100,n_clusters=2)\n",
    "kmeans.fit(data)\n",
    "kmeans.predict([12, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m k_means = KMeans(dimension=\u001b[32m2\u001b[39m,max_iter=\u001b[32m100\u001b[39m,n_clusters=\u001b[32m2\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# apprentissage −> realisation du partitionnement\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mk_means\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# affichage des numeros de clusters affectes aux donnees\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(k_means.get_data_clusters())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mKMeans.fit\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28mself\u001b[39m.centroids = data[np.random.choice(\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.n_clusters, replace=\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mself.centroids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.centroids)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_iter):\n\u001b[32m     15\u001b[39m          \u001b[38;5;66;03m# 3. Assign each point to the closest centroid\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# creation des donnees d’entree\n",
    "# liste de 10 vecteurs de dimension 2 generes aleatoirement\n",
    "X = [np.random.random(2) * 10 for i in range(0,10)]\n",
    "\n",
    "# creation du partitionneur\n",
    "k_means = KMeans(dimension=2,max_iter=100,n_clusters=2)\n",
    "\n",
    "# apprentissage −> realisation du partitionnement\n",
    "k_means.fit(X)\n",
    "\n",
    "# affichage des numeros de clusters affectes aux donnees\n",
    "print(k_means.get_data_clusters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
